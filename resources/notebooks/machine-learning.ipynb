{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "sc = SparkContext('spark://master:7077')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_p = [('John',19),('Smith',29),('Adam',35),('Henry',50)]\n",
    "rdd = sc.parallelize(list_p)\n",
    "ppl = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "DF_ppl = sqlContext.createDataFrame(ppl)\n",
    "DF_ppl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "sc.addFile(url)\n",
    "df = sqlContext.read.csv(SparkFiles.get(\"adult_data.csv\"), header=True, inferSchema= True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockmgr-9606c16b-6140-4700-be16-2de1ea6e74e6\r\n",
      "core-js-banners\r\n",
      "hsperfdata_jovyan\r\n",
      "hsperfdata_root\r\n",
      "liblz4-java-10955810587305368867.so\r\n",
      "liblz4-java-10955810587305368867.so.lck\r\n",
      "npm-2633-a83dd46d\r\n",
      "npm-2675-15b865b9\r\n",
      "npm-2717-cc008c30\r\n",
      "spark-27a36b34-c371-4957-a871-ec841acb74e9\r\n",
      "spark-430bbca1-e81e-4c87-83d0-eb13dd68f3aa\r\n",
      "v8-compile-cache-1000\r\n",
      "yarn--1596992486814-0.07181424149422089\r\n",
      "yarn--1596992492174-0.3233965189961965\r\n",
      "yarn--1596992493231-0.8582299164202789\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying that master is up...\n",
      "Submitting job...\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.0.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "20/08/11 20:16:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/08/11 20:16:48 INFO SparkContext: Running Spark version 3.0.0\n",
      "20/08/11 20:16:48 INFO ResourceUtils: ==============================================================\n",
      "20/08/11 20:16:48 INFO ResourceUtils: Resources for spark.driver:\n",
      "\n",
      "20/08/11 20:16:48 INFO ResourceUtils: ==============================================================\n",
      "20/08/11 20:16:48 INFO SparkContext: Submitted application: job.py\n",
      "20/08/11 20:16:49 INFO SecurityManager: Changing view acls to: jovyan\n",
      "20/08/11 20:16:49 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "20/08/11 20:16:49 INFO SecurityManager: Changing view acls groups to: \n",
      "20/08/11 20:16:49 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/08/11 20:16:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "20/08/11 20:17:14 INFO Utils: Successfully started service 'sparkDriver' on port 36165.\n",
      "20/08/11 20:17:14 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/08/11 20:17:32 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/08/11 20:17:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/08/11 20:17:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/08/11 20:17:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "20/08/11 20:17:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-400ba6e3-9998-4ee9-951f-85d7cdc88bad\n",
      "20/08/11 20:17:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "20/08/11 20:17:33 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/08/11 20:17:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "20/08/11 20:17:35 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "20/08/11 20:17:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://notebook:4041\n",
      "20/08/11 20:17:38 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://master:7077...\n",
      "20/08/11 20:17:40 INFO TransportClientFactory: Successfully created connection to master/192.168.224.3:7077 after 745 ms (0 ms spent in bootstraps)\n",
      "20/08/11 20:17:47 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20200811201745-0002\n",
      "20/08/11 20:17:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46573.\n",
      "20/08/11 20:17:47 INFO NettyBlockTransferService: Server created on notebook:46573\n",
      "20/08/11 20:17:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/08/11 20:17:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, notebook, 46573, None)\n",
      "20/08/11 20:17:47 INFO BlockManagerMasterEndpoint: Registering block manager notebook:46573 with 434.4 MiB RAM, BlockManagerId(driver, notebook, 46573, None)\n",
      "20/08/11 20:17:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, notebook, 46573, None)\n",
      "20/08/11 20:17:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, notebook, 46573, None)\n",
      "20/08/11 20:17:55 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "20/08/11 20:17:55 INFO AsyncEventQueue: Process of event SparkListenerBlockManagerAdded(1597177067958,BlockManagerId(driver, notebook, 46573, None),455501414,Some(455501414),Some(0)) by listener AppStatusListener took 1.7012573s.\n",
      "20/08/11 20:17:57 INFO SparkContext: Added file https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv at https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv with timestamp 1597177077890\n",
      "20/08/11 20:18:00 INFO Utils: Fetching https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv to /tmp/spark-401838f5-233f-4c3c-b23b-d58f007c9e78/userFiles-020c9519-411c-4c5d-b11d-41133aeec844/fetchFileTemp8153438770158293152.tmp\n",
      "20/08/11 20:18:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/notebooks/spark-warehouse').\n",
      "20/08/11 20:18:07 INFO SharedState: Warehouse path is 'file:/notebooks/spark-warehouse'.\n",
      "20/08/11 20:18:09 INFO InMemoryFileIndex: It took 384 ms to list leaf files for 1 paths.\n",
      "20/08/11 20:18:10 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.\n",
      "20/08/11 20:18:24 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/08/11 20:18:24 INFO FileSourceStrategy: Pushed Filters: \n",
      "20/08/11 20:18:24 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "20/08/11 20:18:24 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "20/08/11 20:18:30 INFO CodeGenerator: Code generated in 1705.758 ms\n",
      "20/08/11 20:18:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 174.4 KiB, free 434.2 MiB)\n",
      "20/08/11 20:18:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.2 MiB)\n",
      "20/08/11 20:18:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on notebook:46573 (size: 27.6 KiB, free: 434.4 MiB)\n",
      "20/08/11 20:18:31 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "20/08/11 20:18:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4901311 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/08/11 20:18:32 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "20/08/11 20:18:32 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "20/08/11 20:18:32 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "20/08/11 20:18:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/08/11 20:18:32 INFO DAGScheduler: Missing parents: List()\n",
      "20/08/11 20:18:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/08/11 20:18:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 434.2 MiB)\n",
      "20/08/11 20:18:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 434.2 MiB)\n",
      "20/08/11 20:18:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on notebook:46573 (size: 5.3 KiB, free: 434.4 MiB)\n",
      "20/08/11 20:18:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200\n",
      "20/08/11 20:18:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "20/08/11 20:18:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "20/08/11 20:18:47 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/08/11 20:19:02 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/08/11 20:19:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/08/11 20:19:32 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/08/11 20:19:47 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/08/11 20:20:02 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/08/11 20:20:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/08/11 20:20:32 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/08/11 20:20:47 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/08/11 20:20:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20200811201745-0002/0 on worker-20200811201129-192.168.224.4-36285 (192.168.224.4:36285) with 1 core(s)\n",
      "20/08/11 20:20:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20200811201745-0002/0 on hostPort 192.168.224.4:36285 with 1 core(s), 1024.0 MiB RAM\n",
      "20/08/11 20:20:53 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20200811201745-0002/0 is now RUNNING\n",
      "20/08/11 20:21:02 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/08/11 20:21:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "20/08/11 20:21:09 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.224.4:51200) with ID 0\n",
      "20/08/11 20:21:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.224.4:34063 with 434.4 MiB RAM, BlockManagerId(0, 192.168.224.4, 34063, None)\n",
      "20/08/11 20:21:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.224.4, executor 0, partition 0, PROCESS_LOCAL, 7817 bytes)\n",
      "20/08/11 20:21:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.224.4:34063 (size: 5.3 KiB, free: 434.4 MiB)\n",
      "20/08/11 20:21:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.224.4:34063 (size: 27.6 KiB, free: 434.4 MiB)\n",
      "20/08/11 20:21:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 9879 ms on 192.168.224.4 (executor 0) (1/1)\n",
      "20/08/11 20:21:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/08/11 20:21:20 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 167.561 s\n",
      "20/08/11 20:21:20 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "20/08/11 20:21:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "20/08/11 20:21:20 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 168.059961 s\n",
      "20/08/11 20:21:20 INFO CodeGenerator: Code generated in 106.076 ms\n",
      "20/08/11 20:21:20 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/08/11 20:21:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "20/08/11 20:21:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "20/08/11 20:21:20 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "20/08/11 20:21:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 174.4 KiB, free 434.0 MiB)\n",
      "20/08/11 20:21:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.0 MiB)\n",
      "20/08/11 20:21:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on notebook:46573 (size: 27.6 KiB, free: 434.3 MiB)\n",
      "20/08/11 20:21:20 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "20/08/11 20:21:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4901311 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/08/11 20:21:20 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "20/08/11 20:21:20 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "20/08/11 20:21:20 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "20/08/11 20:21:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/08/11 20:21:20 INFO DAGScheduler: Missing parents: List()\n",
      "20/08/11 20:21:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/08/11 20:21:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.3 KiB, free 434.0 MiB)\n",
      "20/08/11 20:21:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.0 MiB)\n",
      "20/08/11 20:21:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on notebook:46573 (size: 7.7 KiB, free: 434.3 MiB)\n",
      "20/08/11 20:21:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1200\n",
      "20/08/11 20:21:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "20/08/11 20:21:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks\n",
      "20/08/11 20:21:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.224.4, executor 0, partition 0, PROCESS_LOCAL, 7817 bytes)\n",
      "20/08/11 20:21:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.224.4:34063 (size: 7.7 KiB, free: 434.4 MiB)\n",
      "20/08/11 20:21:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.224.4:34063 (size: 27.6 KiB, free: 434.3 MiB)\n",
      "20/08/11 20:21:37 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, 192.168.224.4, executor 0, partition 1, PROCESS_LOCAL, 7817 bytes)\n",
      "20/08/11 20:21:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 16031 ms on 192.168.224.4 (executor 0) (1/2)\n",
      "20/08/11 20:21:43 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 6268 ms on 192.168.224.4 (executor 0) (2/2)\n",
      "20/08/11 20:21:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "20/08/11 20:21:43 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 22.439 s\n",
      "20/08/11 20:21:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "20/08/11 20:21:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "20/08/11 20:21:43 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 22.490454 s\n",
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n",
      "*** DONE ***\n",
      "20/08/11 20:21:45 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "20/08/11 20:21:45 INFO SparkUI: Stopped Spark web UI at http://notebook:4041\n",
      "20/08/11 20:21:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on notebook:46573 in memory (size: 27.6 KiB, free: 434.4 MiB)\n",
      "20/08/11 20:21:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.224.4:34063 in memory (size: 27.6 KiB, free: 434.4 MiB)\n",
      "20/08/11 20:21:46 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "20/08/11 20:21:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "20/08/11 20:21:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/08/11 20:21:47 INFO MemoryStore: MemoryStore cleared\n",
      "20/08/11 20:21:47 INFO BlockManager: BlockManager stopped\n",
      "20/08/11 20:21:47 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/08/11 20:21:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/08/11 20:21:47 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/08/11 20:21:47 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/08/11 20:21:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-401838f5-233f-4c3c-b23b-d58f007c9e78/pyspark-54cb0d3a-f73c-47f8-a789-fa825fa02682\n",
      "20/08/11 20:21:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-b636667a-5c5e-4b9e-8e15-576b688cff00\n",
      "20/08/11 20:21:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-401838f5-233f-4c3c-b23b-d58f007c9e78\n"
     ]
    }
   ],
   "source": [
    "!/jobs/notebook-test/submit.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
